{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def average_ratings(image_data):\n",
    "    total_ratings = len(image_data['ratings'])\n",
    "    if total_ratings > 1:\n",
    "        avg_ratings = {key: sum(rating[key] for rating in image_data['ratings']) / total_ratings \n",
    "                       for key in image_data['ratings'][0]}\n",
    "        return avg_ratings\n",
    "    elif total_ratings == 1:\n",
    "        return image_data['ratings'][0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def update_json(json_data):\n",
    "    updated_images = []\n",
    "    for image in json_data['images']:\n",
    "        if image is None:\n",
    "            continue  # Skip if image is None\n",
    "        if image['numberOfRatings'] == 0:\n",
    "            continue\n",
    "        avg = average_ratings(image)\n",
    "        if avg:\n",
    "            image['ratings'] = avg\n",
    "        updated_images.append(image)\n",
    "    json_data['images'] = updated_images\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_file = 'exported.json'\n",
    "    output_file = 'averaged_image_data.json'\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    updated_data = update_json(data)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(updated_data, f, indent=4)\n",
    "\n",
    "    print(\"Averaged data has been written to\", output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"output.csv\")\n",
    "\n",
    "# Filter rows where the URL starts with \"http://\" or \"https://\"\n",
    "df = df[df['images/Image_URL'].str.startswith(('http://', 'https://'))]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "df.to_csv(\"filtered_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"filtered_output.csv\")\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in df.iterrows():\n",
    "    url = row['images/Image_URL']\n",
    "    if url.startswith(('http://', 'https://')):\n",
    "        try:\n",
    "            print(f\"Downloading image from {url}\")  # Print the URL before making the request\n",
    "            # Send a GET request to download the image\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "            print(f\"Response status code: {response.status_code}\")  # Print the response status code\n",
    "            if response.status_code == 200:\n",
    "                # Read the image from the response content\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                \n",
    "                # Save the image to a file\n",
    "                img_path = f\"images/image_{index}.jpg\"  # You can change the file format if needed\n",
    "                img.save(img_path)\n",
    "                \n",
    "                # Update the row with the image path\n",
    "                df.at[index, 'image_path'] = img_path\n",
    "            else:\n",
    "                print(f\"Failed to download image from {url} at index {index}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image from {url}: {e}\")\n",
    "    else:\n",
    "        print(f\"Invalid URL: {url} at index {index}\")\n",
    "\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"final_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('final_output.csv')\n",
    "\n",
    "df.drop(['images/Image_URL'], axis=1, inplace=True)\n",
    "df.drop(['images/ratings/overall'], axis =1, inplace = True)\n",
    "df.drop(['images/numberOfRatings'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# Drop rows with any empty fields\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "###################\n",
    "#df.drop(df.index[6:],inplace=True)\n",
    "\n",
    "df.rename(columns = {'images/ratings/contrast':'contrast', 'images/ratings/localContrast': 'localContrast', 'images/ratings/entropy': 'entropy', 'images/ratings/localEntropy': 'localEntropy', 'images/ratings/saturation': 'saturation'}, inplace = True) \n",
    "\n",
    "# Write the DataFrame back to a CSV file\n",
    "df.to_csv('cleaned_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 4s 208ms/step - loss: 6.0934 - val_loss: 2.3921\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 3s 178ms/step - loss: 3.3940 - val_loss: 2.1715\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 3s 177ms/step - loss: 2.9624 - val_loss: 2.0408\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 3s 177ms/step - loss: 2.7482 - val_loss: 1.9685\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 3s 177ms/step - loss: 2.5952 - val_loss: 1.9572\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 3s 178ms/step - loss: 2.6853 - val_loss: 2.3141\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 3s 177ms/step - loss: 2.5430 - val_loss: 1.8517\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 3s 177ms/step - loss: 2.3962 - val_loss: 2.1294\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 3s 176ms/step - loss: 2.3154 - val_loss: 1.8800\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 3s 175ms/step - loss: 2.1872 - val_loss: 2.1993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a7063de040>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('cleaned_output.csv')\n",
    "\n",
    "# Load images and preprocess\n",
    "images = []\n",
    "for img_path in data['image_path']:\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))  # Resize images to a fixed size\n",
    "    images.append(img)\n",
    "images = np.array(images)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "images = images / 255.0  # Normalize pixel values to range [0, 1]\n",
    "\n",
    "# Define target variables\n",
    "y = data[['contrast', 'entropy', 'localContrast', 'localEntropy', 'saturation']].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(5)  # Output layer with 5 neurons for 5 target variables\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "Contrast: 4\n",
      "Entropy: 3\n",
      "Local Contrast: 3\n",
      "Local Entropy: 3\n",
      "Saturation: 2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the image\n",
    "image_path = 'images/image_0.jpg'  # Change this to the path of your image\n",
    "img = cv2.imread(image_path)\n",
    "resized_img = cv2.resize(img, (100, 100))  # Resize the image to match the input shape of the model\n",
    "\n",
    "# Normalize the image\n",
    "scaler = MinMaxScaler()\n",
    "normalized_img = scaler.fit_transform(resized_img.reshape(1, -1))\n",
    "\n",
    "# Reshape the image to match the input shape of the model\n",
    "normalized_img = normalized_img.reshape(1, 100, 100, 3)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(normalized_img)\n",
    "\n",
    "# Round the predicted values to remove decimals\n",
    "rounded_prediction = np.round(prediction)\n",
    "flattened_prediction = rounded_prediction.flatten().astype(int)\n",
    "\n",
    "\n",
    "print(\"Predicted values:\")\n",
    "print(\"Contrast:\", flattened_prediction[0])\n",
    "print(\"Entropy:\", flattened_prediction[1])\n",
    "print(\"Local Contrast:\", flattened_prediction[2])\n",
    "print(\"Local Entropy:\", flattened_prediction[3])\n",
    "print(\"Saturation:\", flattened_prediction[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS Contrast: 1\n",
      "Entropy: 4\n",
      "Local Contrast: 1\n",
      "Local Entropy: 4\n",
      "Mean Saturation: 2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def rgb_to_hsv(image):\n",
    "   # Normalize pixel values to the range [0, 1]\n",
    "    image_normalized = image.astype(np.float32) / 255.0\n",
    "\n",
    "    # Extract R, G, B components\n",
    "    R, G, B = image_normalized[:, :, 0], image_normalized[:, :, 1], image_normalized[:, :, 2]\n",
    "\n",
    "    # Compute Value (V)\n",
    "    V = np.max(image_normalized, axis=2)\n",
    "\n",
    "    # Compute Saturation (S)\n",
    "    denominator = np.where(V != 0, V, 1.0)\n",
    "    S = (V - np.min(image_normalized, axis=2)) / denominator\n",
    "\n",
    "    # Compute Hue (H)\n",
    "    delta_R = (V - R) / (6 * denominator + 1e-10) + 1.0\n",
    "    delta_G = (V - G) / (6 * denominator + 1e-10) + 1.0\n",
    "    delta_B = (V - B) / (6 * denominator + 1e-10) + 1.0\n",
    "\n",
    "    H = np.where(V == R, delta_B - delta_G, np.where(V == G, 2.0 + delta_R - delta_B, 4.0 + delta_G - delta_R))\n",
    "    H = (H / 6.0) % 1.0\n",
    "\n",
    "    return H * 360, S, V\n",
    "\n",
    "\n",
    "def calculate_entropy(intensity_channel):\n",
    "    # Calculate histogram of intensity values\n",
    "    hist, _ = np.histogram(intensity_channel, bins=256, range=(0, 1))\n",
    "\n",
    "    # Compute probability distribution\n",
    "    prob_distribution = hist / np.sum(hist)\n",
    "\n",
    "    # Remove zero probabilities to avoid NaN in the entropy calculation\n",
    "    non_zero_probs = prob_distribution[prob_distribution > 0]\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_local_entropy_partial(intensity_channel, window_size=3):\n",
    "    height, width = intensity_channel.shape\n",
    "\n",
    "    # Calculate the number of non-overlapping blocks in height and width\n",
    "    block_height = height // window_size\n",
    "    block_width = width // window_size\n",
    "\n",
    "    # Reshape the intensity channel to a 4D array with dimensions for block_height and block_width\n",
    "    blocks = intensity_channel[:block_height * window_size, :block_width * window_size] \\\n",
    "        .reshape(block_height, window_size, block_width, window_size)\n",
    "\n",
    "    # Calculate histogram for all blocks\n",
    "    hist, _ = np.histogram(blocks, bins=256, range=(0, 1))\n",
    "\n",
    "    # Compute probability distribution\n",
    "    prob_distribution = hist / np.sum(hist)\n",
    "\n",
    "    # Remove zero probabilities to avoid NaN in the entropy calculation\n",
    "    non_zero_probs = np.where(prob_distribution > 0, prob_distribution, 1.0)\n",
    "\n",
    "    # Calculate entropy for all blocks\n",
    "    local_entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n",
    "\n",
    "    return local_entropy\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rms_contrast(intensity_channel):\n",
    "    # Calculate the standard deviation of the intensity channel\n",
    "    std_intensity = np.std(intensity_channel)\n",
    "\n",
    "    return std_intensity\n",
    "\n",
    "\n",
    "def calculate_local_contrast(intensity_channel, window_size=3):\n",
    "    height, width = intensity_channel.shape\n",
    "\n",
    "    # Calculate the number of non-overlapping blocks in height and width\n",
    "    block_height = height // window_size\n",
    "    block_width = width // window_size\n",
    "\n",
    "    # Reshape the intensity channel to a 4D array with dimensions for block_height and block_width\n",
    "    blocks = intensity_channel[:block_height * window_size, :block_width * window_size] \\\n",
    "        .reshape(block_height, window_size, block_width, window_size)\n",
    "\n",
    "    local_contrast = np.zeros((block_height, block_width))\n",
    "\n",
    "    for i in range(block_height):\n",
    "        for j in range(block_width):\n",
    "            block = blocks[i, :, j, :]\n",
    "\n",
    "            # Calculate standard deviation for the block\n",
    "            local_contrast[i, j] = np.std(block)\n",
    "\n",
    "    # Calculate the mean of local contrasts\n",
    "    local_contrast_mean = np.mean(local_contrast)\n",
    "            \n",
    "    return local_contrast_mean\n",
    "\n",
    "\n",
    "\n",
    "def normalize_value(value, min_val, max_val, new_min=1, new_max=5):\n",
    "    normalized_value = ((value - min_val) / (max_val - min_val)) * (new_max - new_min) + new_min\n",
    "    return normalized_value\n",
    "\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Load the input image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert BGR to RGB (OpenCV loads images in BGR format)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert RGB to HSV\n",
    "    H, S, V = rgb_to_hsv(image_rgb)\n",
    "\n",
    "    # Calculate RMS contrast using the Intensity (V) component\n",
    "    rms_contrast_value = calculate_rms_contrast(V)\n",
    "    rms_contrast_normalized = normalize_value(rms_contrast_value, 0, 255)\n",
    "\n",
    "    # Calculate the mean value of the S component\n",
    "    mean_saturation = np.mean(S)\n",
    "    mean_saturation_normalized = normalize_value(mean_saturation, 0, 1)\n",
    "\n",
    "    # Calculate entropy based on the Intensity (V) component\n",
    "    entropy_I = calculate_entropy(V)\n",
    "    entropy_normalized = normalize_value(entropy_I, 0, -np.log2(1/256))\n",
    "\n",
    "    # Calculate local entropy\n",
    "    loc_ent = calculate_local_entropy_partial(V)\n",
    "    loc_ent_normalized = normalize_value(loc_ent, 0, -np.log2(1/256))\n",
    "\n",
    "    # Calculate local contrast\n",
    "    local_contrast = calculate_local_contrast(V, 5)\n",
    "    local_contrast_normalized = normalize_value(local_contrast, 0, 255)\n",
    "\n",
    "    return [rms_contrast_normalized, entropy_normalized, local_contrast_normalized,loc_ent_normalized, mean_saturation_normalized]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_image = \"images/image_0.jpg\"  # Change this to the path of your input image\n",
    "\n",
    "    # Process the input image\n",
    "    calc_results = process_image(input_image) \n",
    "\n",
    "    # Print the results\n",
    "    #print(\"Image Path:\", results[0])\n",
    "    print(\"RMS Contrast:\", np.round(calc_results[0]).astype(int) )\n",
    "    print(\"Entropy:\", np.round(calc_results[1]).astype(int))\n",
    "    print(\"Local Contrast:\", np.round(calc_results[2]).astype(int))\n",
    "    print(\"Local Entropy:\", np.round(calc_results[3]).astype(int))\n",
    "    print(\"Mean Saturation:\", np.round(calc_results[4]).astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity percentages: [25.04346089328037, 68.34312688622693, 33.341766810824005, 68.34312688622693, 98.82759128092327]\n",
      "Weights: [0.08521109188372851, 0.23253944371107982, 0.11344631508360298, 0.23253944371107982, 0.3362637056105088]\n",
      "Final value: 2.920857121828735\n"
     ]
    }
   ],
   "source": [
    "def calculate_similarity(pArr, cArr):\n",
    "  \"\"\"\n",
    "  This function calculates the similarity between two elements in percentages.\n",
    "  Args:\n",
    "      pArr: The first array.\n",
    "      cArr: The second array.\n",
    "  Returns:\n",
    "      A list of similarity percentages for each element.\n",
    "  \"\"\"\n",
    "  similarity = []\n",
    "  for i in range(len(pArr)):\n",
    "    if pArr[i] == cArr[i]:\n",
    "      similarity.append(100)\n",
    "    else:\n",
    "      max_val = max(pArr[i], cArr[i])\n",
    "      min_val = min(pArr[i], cArr[i])\n",
    "      difference = max_val - min_val\n",
    "      similarity.append((1 - difference/max_val) * 100)\n",
    "  return similarity\n",
    "\n",
    "def calculate_weights(similarity):\n",
    "  \"\"\"\n",
    "  This function calculates the weights based on the similarity percentages.\n",
    "  Args:\n",
    "      similarity: A list of similarity percentages.\n",
    "  Returns:\n",
    "      A list of weights.\n",
    "  \"\"\"\n",
    "  total_similarity = sum(similarity)\n",
    "  weights = [sim / total_similarity for sim in similarity]\n",
    "  return weights\n",
    "\n",
    "def calculate_final_value(cArr, weights):\n",
    "  \"\"\"\n",
    "  This function calculates the final value using the weights and calculated values.\n",
    "  Args:\n",
    "      cArr: The second array containing calculated values.\n",
    "      weights: A list of weights.\n",
    "  Returns:\n",
    "      The final value.\n",
    "  \"\"\"\n",
    "  final_value = 0\n",
    "  for i in range(len(cArr)):\n",
    "    final_value += cArr[i] * weights[i]\n",
    "  return final_value\n",
    "\n",
    "# Example usage\n",
    "pArr = flattened_prediction\n",
    "cArr = calc_results\n",
    "\n",
    "similarity = calculate_similarity(pArr, cArr)\n",
    "weights = calculate_weights(similarity)\n",
    "final_value = calculate_final_value(cArr, weights)\n",
    "\n",
    "print(\"Similarity percentages:\", similarity)\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Final value:\", final_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
