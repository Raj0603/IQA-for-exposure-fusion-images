import pandas as pd
import requests
from io import BytesIO
from PIL import Image

# Read the CSV file
df = pd.read_csv("filtered_output.csv")

# Iterate over each row
for index, row in df.iterrows():
    url = row['images/Image_URL']
    if url.startswith(('http://', 'https://')):
        try:
            # Send a GET request to download the image
            response = requests.get(url)
            if response.status_code == 200:
                # Read the image from the response content
                img = Image.open(BytesIO(response.content))
                
                # Save the image to a file
                img_path = f"image_{index}.jpg"  # You can change the file format if needed
                img.save(img_path)
                
                # Update the row with the image path
                df.at[index, 'image_path'] = img_path
            else:
                print(f"Failed to download image from {url}")
        except Exception as e:
            print(f"Error downloading image from {url}: {e}")
    else:
        print(f"Invalid URL: {url}")

# Save the updated DataFrame to a new CSV file
df.to_csv("final_output.csv", index=False)



////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////



import pandas as pd

# Read the CSV file
df = pd.read_csv("output.csv")

# Filter rows where the URL starts with "http://" or "https://"
df = df[df['images/Image_URL'].str.startswith(('http://', 'https://'))]

# Save the filtered DataFrame to a new CSV file
df.to_csv("filtered_output.csv", index=False)


///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////



import pandas as pd
import requests
from io import BytesIO
from PIL import Image

# Read the CSV file
df = pd.read_csv("filtered_output.csv")

# Iterate over each row
for index, row in df.iterrows():
    url = row['images/Image_URL']
    if url.startswith(('http://', 'https://')):
        try:
            print(f"Downloading image from {url}")  # Print the URL before making the request
            # Send a GET request to download the image
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
            response = requests.get(url, headers=headers, timeout=10)

            print(f"Response status code: {response.status_code}")  # Print the response status code
            if response.status_code == 200:
                # Read the image from the response content
                img = Image.open(BytesIO(response.content))
                
                # Save the image to a file
                img_path = f"img/image_{index}.jpg"  # You can change the file format if needed
                img.save(img_path)
                
                # Update the row with the image path
                df.at[index, 'image_path'] = img_path
            else:
                print(f"Failed to download image from {url} at index {index}")
        except Exception as e:
            print(f"Error downloading image from {url}: {e}")
    else:
        print(f"Invalid URL: {url} at index {index}")


# Save the updated DataFrame to a new CSV file
df.to_csv("final_output.csv", index=False)



///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////



import pandas as pd
import cv2
import numpy as np
from skimage.feature import greycomatrix, greycoprops
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Load dataset
data = pd.read_csv("cleaned_output.csv")

# Function to extract image features
def extract_features(image_path):
    features = process_image(image_path)
    return features

# Function to extract image features
def process_image(image_path):
    # Load the input image
    image = cv2.imread(image_path)

    # Convert BGR to RGB (OpenCV loads images in BGR format)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Convert RGB to HSI
    H, S, I = rgb_to_hsi(image_rgb)

    # Calculate RMS contrast using the Intensity (I) component
    rms_contrast_value = calculate_rms_contrast(I)

    # Calculate local contrast
    local_contrast_value = calculate_local_contrast(I)

    # Calculate local entropy
    local_entropy_value = calculate_local_entropy(I)

    # Calculate the mean value of the S component
    mean_saturation = np.mean(S)

    # Calculate entropy based on the Intensity (I) component
    entropy_I = calculate_entropy(I)

    return [rms_contrast_value, mean_saturation, entropy_I, local_entropy_value, local_contrast_value]

def rgb_to_hsi(image):
    # Normalize pixel values to the range [0, 1]
    image_normalized = image.astype(np.float32) / 255.0

    # Extract R, G, B components
    R, G, B = image_normalized[:, :, 0], image_normalized[:, :, 1], image_normalized[:, :, 2]

    # Compute Intensity (I)
    I = (R + G + B) / 3.0

    # Compute Saturation (S)
    minimum = np.minimum(np.minimum(R, G), B)
    S = 1 - (3 / (R + G + B + 0.001) * minimum)

    # Compute Hue (H)
    numerator = 0.5 * ((R - G) + (R - B))
    denominator = np.sqrt((R - G)**2 + (R - B) * (G - B))
    theta = np.arccos(np.clip(numerator / (denominator + 1e-10), -1.0, 1.0))
    H = np.degrees(theta)
    H[B > G] = 360 - H[B > G]

    return H, S, I


def calculate_entropy(intensity_channel):
    # Calculate histogram of intensity values
    hist, _ = np.histogram(intensity_channel, bins=256, range=(0, 1))

    # Compute probability distribution
    prob_distribution = hist / np.sum(hist)

    # Remove zero probabilities to avoid NaN in the entropy calculation
    non_zero_probs = prob_distribution[prob_distribution > 0]

    # Calculate entropy
    entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))

    return entropy

def calculate_local_entropy(intensity_channel, window_size=3):
    half_window = window_size // 2

    # Pad the intensity_channel to handle borders
    intensity_channel_padded = np.pad(intensity_channel, ((half_window, half_window), (half_window, half_window)), mode='constant')

    # Calculate histogram of the entire intensity channel
    hist, _ = np.histogram(intensity_channel, bins=256, range=(0, 1))

    # Compute probability distribution
    prob_distribution = hist / np.sum(hist)

    local_entropy = np.zeros_like(intensity_channel)

    for i in range(intensity_channel.shape[0]):
        for j in range(intensity_channel.shape[1]):
            local_region = intensity_channel_padded[i:i + window_size, j:j + window_size]
            local_hist, _ = np.histogram(local_region, bins=256, range=(0, 1))
            local_prob_distribution = local_hist / np.sum(local_hist)

            # Remove zero probabilities to avoid NaN in the entropy calculation
            non_zero_probs = local_prob_distribution[local_prob_distribution > 0]

            # Calculate entropy
            local_entropy[i, j] = -np.sum(non_zero_probs * np.log2(non_zero_probs))

    return local_entropy

def calculate_rms_contrast(intensity_channel):
    # Calculate the standard deviation of the intensity channel
    std_intensity = np.std(intensity_channel)

    return std_intensity

#extra constrast function
def rms_contrast(intensity_channel):
       
    # Calculate RMS contrast
    contrast = np.sqrt(np.mean(intensity_channel**2))
    
    return contrast

def calculate_local_contrast(intensity_channel, window_size=3):
    half_window = window_size // 2

    # Pad the intensity_channel to handle borders
    intensity_channel_padded = np.pad(intensity_channel, ((half_window, half_window), (half_window, half_window)), mode='constant')

    local_contrast = np.zeros_like(intensity_channel)

    for i in range(intensity_channel.shape[0]):
        for j in range(intensity_channel.shape[1]):
            local_region = intensity_channel_padded[i:i + window_size, j:j + window_size]

            # Calculate standard deviation for local contrast
            local_contrast[i, j] = np.std(local_region)

    return local_contrast

# Extract features for all images
features = []
ratings = []

for index, row in data.iterrows():
    features.append(extract_features(row['image_path']))
    ratings.append([row['saturation'], row['contrast'], row['localContrast'], row['entropy'], row['localEntropy']])

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, ratings, test_size=0.2, random_state=42)

# Define and train the model
model = RandomForestRegressor()
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
