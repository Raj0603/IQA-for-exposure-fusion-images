{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 6s 217ms/step - loss: 6.5797 - val_loss: 1.5265\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 5s 189ms/step - loss: 2.6428 - val_loss: 1.4624\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 5s 191ms/step - loss: 2.4259 - val_loss: 1.3826\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 5s 194ms/step - loss: 2.0631 - val_loss: 1.3927\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 5s 199ms/step - loss: 2.1863 - val_loss: 1.3972\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 5s 198ms/step - loss: 2.0133 - val_loss: 1.4319\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 5s 185ms/step - loss: 2.1566 - val_loss: 1.4097\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 5s 198ms/step - loss: 1.9004 - val_loss: 1.4778\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 5s 188ms/step - loss: 1.7810 - val_loss: 1.4912\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 5s 182ms/step - loss: 1.7376 - val_loss: 1.3697\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import cv2\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('cleaned_output.csv')\n",
    "\n",
    "# Load images and preprocess\n",
    "images = []\n",
    "for img_path in data['image_path']:\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))  # Resize images to a fixed size\n",
    "    images.append(img)\n",
    "images = np.array(images)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "images = images / 255.0  # Normalize pixel values to range [0, 1]\n",
    "\n",
    "# Define target variables\n",
    "y = data[['contrast', 'entropy', 'localContrast', 'localEntropy', 'saturation']].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(5)  # Output layer with 5 neurons for 5 target variables\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "model.save('image_quality_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def rgb_to_hsv(image):\n",
    "   # Normalize pixel values to the range [0, 1]\n",
    "    image_normalized = image.astype(np.float32) / 255.0\n",
    "\n",
    "    # Extract R, G, B components\n",
    "    R, G, B = image_normalized[:, :, 0], image_normalized[:, :, 1], image_normalized[:, :, 2]\n",
    "\n",
    "    # Compute Value (V)\n",
    "    V = np.max(image_normalized, axis=2)\n",
    "\n",
    "    # Compute Saturation (S)\n",
    "    denominator = np.where(V != 0, V, 1.0)\n",
    "    S = (V - np.min(image_normalized, axis=2)) / denominator\n",
    "\n",
    "    # Compute Hue (H)\n",
    "    delta_R = (V - R) / (6 * denominator + 1e-10) + 1.0\n",
    "    delta_G = (V - G) / (6 * denominator + 1e-10) + 1.0\n",
    "    delta_B = (V - B) / (6 * denominator + 1e-10) + 1.0\n",
    "\n",
    "    H = np.where(V == R, delta_B - delta_G, np.where(V == G, 2.0 + delta_R - delta_B, 4.0 + delta_G - delta_R))\n",
    "    H = (H / 6.0) % 1.0\n",
    "\n",
    "    return H * 360, S, V\n",
    "\n",
    "\n",
    "def calculate_entropy(intensity_channel):\n",
    "    # Calculate histogram of intensity values\n",
    "    hist, _ = np.histogram(intensity_channel, bins=256, range=(0, 1))\n",
    "\n",
    "    # Compute probability distribution\n",
    "    prob_distribution = hist / np.sum(hist)\n",
    "\n",
    "    # Remove zero probabilities to avoid NaN in the entropy calculation\n",
    "    non_zero_probs = prob_distribution[prob_distribution > 0]\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_local_entropy_partial(intensity_channel, window_size=3):\n",
    "    height, width = intensity_channel.shape\n",
    "\n",
    "    # Calculate the number of non-overlapping blocks in height and width\n",
    "    block_height = height // window_size\n",
    "    block_width = width // window_size\n",
    "\n",
    "    # Reshape the intensity channel to a 4D array with dimensions for block_height and block_width\n",
    "    blocks = intensity_channel[:block_height * window_size, :block_width * window_size] \\\n",
    "        .reshape(block_height, window_size, block_width, window_size)\n",
    "\n",
    "    # Calculate histogram for all blocks\n",
    "    hist, _ = np.histogram(blocks, bins=256, range=(0, 1))\n",
    "\n",
    "    # Compute probability distribution\n",
    "    prob_distribution = hist / np.sum(hist)\n",
    "\n",
    "    # Remove zero probabilities to avoid NaN in the entropy calculation\n",
    "    non_zero_probs = np.where(prob_distribution > 0, prob_distribution, 1.0)\n",
    "\n",
    "    # Calculate entropy for all blocks\n",
    "    local_entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n",
    "\n",
    "    return local_entropy\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rms_contrast(intensity_channel):\n",
    "    # Calculate the standard deviation of the intensity channel\n",
    "    std_intensity = np.std(intensity_channel)\n",
    "\n",
    "    return std_intensity\n",
    "\n",
    "\n",
    "def calculate_local_contrast(intensity_channel, window_size=3):\n",
    "    height, width = intensity_channel.shape\n",
    "\n",
    "    # Calculate the number of non-overlapping blocks in height and width\n",
    "    block_height = height // window_size\n",
    "    block_width = width // window_size\n",
    "\n",
    "    # Reshape the intensity channel to a 4D array with dimensions for block_height and block_width\n",
    "    blocks = intensity_channel[:block_height * window_size, :block_width * window_size] \\\n",
    "        .reshape(block_height, window_size, block_width, window_size)\n",
    "\n",
    "    local_contrast = np.zeros((block_height, block_width))\n",
    "\n",
    "    for i in range(block_height):\n",
    "        for j in range(block_width):\n",
    "            block = blocks[i, :, j, :]\n",
    "\n",
    "            # Calculate standard deviation for the block\n",
    "            local_contrast[i, j] = np.std(block)\n",
    "\n",
    "    # Calculate the mean of local contrasts\n",
    "    local_contrast_mean = np.mean(local_contrast)\n",
    "            \n",
    "    return local_contrast_mean\n",
    "\n",
    "\n",
    "\n",
    "def normalize_value(value, min_val, max_val, new_min=1, new_max=5):\n",
    "    normalized_value = ((value - min_val) / (max_val - min_val)) * (new_max - new_min) + new_min\n",
    "    return normalized_value\n",
    "\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Load the input image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert BGR to RGB (OpenCV loads images in BGR format)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert RGB to HSV\n",
    "    H, S, V = rgb_to_hsv(image_rgb)\n",
    "\n",
    "    # Calculate RMS contrast using the Intensity (V) component\n",
    "    rms_contrast_value = calculate_rms_contrast(V)\n",
    "    rms_contrast_normalized = normalize_value(rms_contrast_value, 0, 255)\n",
    "\n",
    "    # Calculate the mean value of the S component\n",
    "    mean_saturation = np.mean(S)\n",
    "    mean_saturation_normalized = normalize_value(mean_saturation, 0, 1)\n",
    "\n",
    "    # Calculate entropy based on the Intensity (V) component\n",
    "    entropy_I = calculate_entropy(V)\n",
    "    entropy_normalized = normalize_value(entropy_I, 0, -np.log2(1/256))\n",
    "\n",
    "    # Calculate local entropy\n",
    "    loc_ent = calculate_local_entropy_partial(V)\n",
    "    loc_ent_normalized = normalize_value(loc_ent, 0, -np.log2(1/256))\n",
    "\n",
    "    # Calculate local contrast\n",
    "    local_contrast = calculate_local_contrast(V, 5)\n",
    "    local_contrast_normalized = normalize_value(local_contrast, 0, 255)\n",
    "\n",
    "    return [rms_contrast_normalized, entropy_normalized, local_contrast_normalized,loc_ent_normalized, mean_saturation_normalized]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(pArr, cArr):\n",
    "  similarity = []\n",
    "  for i in range(len(pArr)):\n",
    "    if pArr[i] == cArr[i]:\n",
    "      similarity.append(100)\n",
    "    else:\n",
    "      max_val = max(pArr[i], cArr[i])\n",
    "      min_val = min(pArr[i], cArr[i])\n",
    "      difference = max_val - min_val\n",
    "      similarity.append((1 - difference/max_val) * 100)\n",
    "  return similarity\n",
    "\n",
    "def calculate_weights(similarity):\n",
    "  total_similarity = sum(similarity)\n",
    "  weights = [sim / total_similarity for sim in similarity]\n",
    "  return weights\n",
    "\n",
    "def calculate_final_value(cArr, weights):\n",
    "  final_value = 0\n",
    "  for i in range(len(cArr)):\n",
    "    final_value += cArr[i] * weights[i]\n",
    "  return final_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e99037683961>:55: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  pil_image = pil_image.resize((new_width, new_height), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "\n",
    "# Create GUI\n",
    "class ImageQualityGUI:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Image Quality Predictor\")\n",
    "        self.root.geometry(\"600x400\")\n",
    "\n",
    "        self.model = load_model('image_quality_model.keras')\n",
    "\n",
    "        self.canvas = tk.Canvas(self.root, width=300, height=300)\n",
    "        self.canvas.pack()\n",
    "\n",
    "        self.btn_browse = tk.Button(self.root, text=\"Browse Image\", command=self.browse_image)\n",
    "        self.btn_browse.pack()\n",
    "\n",
    "        self.lbl_results = tk.Label(self.root, text=\"\")\n",
    "        self.lbl_results.pack()\n",
    "\n",
    "    def browse_image(self):\n",
    "        # Clear previous results\n",
    "        for widget in self.root.winfo_children():\n",
    "            if isinstance(widget, (tk.Frame, tk.Label)):\n",
    "                widget.destroy()\n",
    "\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png\")])\n",
    "        if file_path:\n",
    "            image = cv2.imread(file_path)\n",
    "            image_resized = cv2.resize(image, (100, 100))\n",
    "            image = np.expand_dims(image_resized, axis=0)\n",
    "            image = image / 255.0\n",
    "\n",
    "            # Predict quality\n",
    "            prediction = self.model.predict(image)\n",
    "\n",
    "            # Display image\n",
    "            image_rgb = cv2.cvtColor(cv2.imread(file_path), cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(image_rgb)\n",
    "            w, h = pil_image.size\n",
    "            aspect_ratio = w / h\n",
    "\n",
    "            # Resize the image to fit the canvas\n",
    "            if aspect_ratio > 1:\n",
    "                new_width = 500\n",
    "                new_height = int(500 / aspect_ratio)\n",
    "            else:\n",
    "                new_width = int(500 * aspect_ratio)\n",
    "                new_height = 500\n",
    "\n",
    "            pil_image = pil_image.resize((new_width, new_height), PIL.Image.Resampling.LANCZOS)\n",
    "            image_tk = ImageTk.PhotoImage(pil_image)\n",
    "            self.canvas.config(width=new_width, height=new_height)\n",
    "            self.canvas.create_image(0, 0, anchor=tk.NW, image=image_tk)\n",
    "            self.canvas.image = image_tk  # Keep a reference to prevent garbage collection\n",
    "\n",
    "            # Process image for additional features\n",
    "            calc_results = process_image(file_path)\n",
    "\n",
    "            # Calculate similarity and weights\n",
    "            similarity = calculate_similarity(prediction[0], calc_results)\n",
    "            weights = calculate_weights(similarity)\n",
    "            final_value = calculate_final_value(calc_results, weights)\n",
    "\n",
    "            # Display results in columns\n",
    "            results_frame = tk.Frame(self.root)\n",
    "            results_frame.pack()\n",
    "\n",
    "            # Predicted quality column\n",
    "            predicted_quality_frame = tk.Frame(results_frame)\n",
    "            predicted_quality_frame.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "            lbl_predicted_quality = tk.Label(predicted_quality_frame, text=\"Predicted Quality:\")\n",
    "            lbl_predicted_quality.pack()\n",
    "\n",
    "            for i, label in enumerate(['Contrast', 'Entropy', 'Local Contrast', 'Local Entropy', 'Saturation']):\n",
    "                lbl_result = tk.Label(predicted_quality_frame, text=f\"{label}: {prediction[0][i]}\")\n",
    "                lbl_result.pack(anchor=tk.W)\n",
    "\n",
    "            # Calculated features column\n",
    "            calculated_features_frame = tk.Frame(results_frame)\n",
    "            calculated_features_frame.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "            lbl_calculated_features = tk.Label(calculated_features_frame, text=\"Calculated Features:\")\n",
    "            lbl_calculated_features.pack()\n",
    "\n",
    "            for i, label in enumerate(['RMS Contrast', 'Entropy', 'Local Contrast', 'Local Entropy', 'Mean Saturation']):\n",
    "                lbl_result = tk.Label(calculated_features_frame, text=f\"{label}: {calc_results[i]}\")\n",
    "                lbl_result.pack(anchor=tk.W)\n",
    "\n",
    "            # Similarity percentages column\n",
    "            similarity_frame = tk.Frame(results_frame)\n",
    "            similarity_frame.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "            lbl_similarity = tk.Label(similarity_frame, text=\"Similarity percentages:\")\n",
    "            lbl_similarity.pack()\n",
    "\n",
    "            for i, label in enumerate(['Contrast', 'Entropy', 'Local Contrast', 'Local Entropy', 'Saturation']):\n",
    "                lbl_result = tk.Label(similarity_frame, text=f\"{label}: {similarity[i]}\")\n",
    "                lbl_result.pack(anchor=tk.W)\n",
    "\n",
    "            # Weights column\n",
    "            weights_frame = tk.Frame(results_frame)\n",
    "            weights_frame.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "            lbl_weights = tk.Label(weights_frame, text=\"Weights:\")\n",
    "            lbl_weights.pack()\n",
    "\n",
    "            for i, label in enumerate(['Contrast', 'Entropy', 'Local Contrast', 'Local Entropy', 'Saturation']):\n",
    "                lbl_result = tk.Label(weights_frame, text=f\"{label}: {weights[i]}\")\n",
    "                lbl_result.pack(anchor=tk.W)\n",
    "\n",
    "            # Final value\n",
    "            lbl_final_value = tk.Label(self.root, text=f\"Final value: {final_value}\")\n",
    "            lbl_final_value.pack()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ImageQualityGUI(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
